{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome! \u00b6 Welcome to the Istio 0 to 60 workshop! On this site you will find the hands-on labs for the workshop. In the first lab, we walk you through accessing and configuring your lab environment . Let's begin.","title":"Welcome!"},{"location":"#welcome","text":"Welcome to the Istio 0 to 60 workshop! On this site you will find the hands-on labs for the workshop. In the first lab, we walk you through accessing and configuring your lab environment . Let's begin.","title":"Welcome!"},{"location":"dashboards/","text":"Observability \u00b6","title":"Observability"},{"location":"dashboards/#observability","text":"","title":"Observability"},{"location":"environment/","text":"Lab environment \u00b6 An environment has been provisioned for you on Google Cloud Platform (GCP), consisting mainly of a Kubernetes cluster using Google Kubernetes Engine (GKE). Log in to GCP \u00b6 Log in to GCP using credentials provided by your instructor. [instructions tbd] Select your project \u00b6 Select the project you have been assigned. [instructions tbd] Launch the Cloud Shell \u00b6 The Google Cloud Shell will serve as your terminal environment for these labs. [instructions tbd] Verify cluster access \u00b6 Check that kubectl is installed kubectl version Verify that your kubernetes context is set to your cluster kubectl config get-contexts List namespaces kubectl get ns Next \u00b6 Now that we have access to our environment and to our Kubernetes cluster, we can proceed to install Istio.","title":"Lab environment"},{"location":"environment/#lab-environment","text":"An environment has been provisioned for you on Google Cloud Platform (GCP), consisting mainly of a Kubernetes cluster using Google Kubernetes Engine (GKE).","title":"Lab environment"},{"location":"environment/#log-in-to-gcp","text":"Log in to GCP using credentials provided by your instructor. [instructions tbd]","title":"Log in to GCP"},{"location":"environment/#select-your-project","text":"Select the project you have been assigned. [instructions tbd]","title":"Select your project"},{"location":"environment/#launch-the-cloud-shell","text":"The Google Cloud Shell will serve as your terminal environment for these labs. [instructions tbd]","title":"Launch the Cloud Shell"},{"location":"environment/#verify-cluster-access","text":"Check that kubectl is installed kubectl version Verify that your kubernetes context is set to your cluster kubectl config get-contexts List namespaces kubectl get ns","title":"Verify cluster access"},{"location":"environment/#next","text":"Now that we have access to our environment and to our Kubernetes cluster, we can proceed to install Istio.","title":"Next"},{"location":"ingress/","text":"Ingress \u00b6","title":"Ingress"},{"location":"ingress/#ingress","text":"","title":"Ingress"},{"location":"install/","text":"Install Istio \u00b6 In this lab you will install Istio. Download Istio \u00b6 Run the following command from your home directory. curl -L https://istio.io/downloadIstio | sh - Navigate into the directory created by the above command. cd istio-1.12.2 Add istioctl to your PATH \u00b6 The istioctl CLI is located in the bin/ subdirectory. Note Cloud Shell only preserves files located inside your home directory across sessions. This means that if you install a binary to a PATH such as /usr/local/bin , chances are tomorrow that file will no longer be there! As a workaround, you will add ${HOME}/bin to your PATH and place the binary there. Create a bin subdirectory in your home directory: mkdir ~/bin Copy the CLI to that subdirectory: cp ./bin/istioctl ~/bin Add your home bin subdirectory to your PATH echo \"export PATH= $HOME /bin: $PATH \" >> ~/.bashrc source ~/.bashrc Verify that istioctl is installed with: istioctl version With the CLI installed, proceed to install Istio to Kubernetes. Install Istio \u00b6 Istio can be installed directly with the cli: istioctl install When prompted, enter y to proceed to install Istio. Take a moment to learn more about Istio installation profiles . Verify that Istio is installed \u00b6 List Kubernetes namespaces and note the new namespace istio-system kubectl get ns Verify that the istiod controller pod is running in that namespace kubectl get pod -n istio-system Re-run istioctl version . The output should include a control plane version, indicating that istio is indeed present in the cluster. Next \u00b6 With Istio installed, we are ready to deploy an application to the mesh.","title":"Install Istio"},{"location":"install/#install-istio","text":"In this lab you will install Istio.","title":"Install Istio"},{"location":"install/#download-istio","text":"Run the following command from your home directory. curl -L https://istio.io/downloadIstio | sh - Navigate into the directory created by the above command. cd istio-1.12.2","title":"Download Istio"},{"location":"install/#add-istioctl-to-your-path","text":"The istioctl CLI is located in the bin/ subdirectory. Note Cloud Shell only preserves files located inside your home directory across sessions. This means that if you install a binary to a PATH such as /usr/local/bin , chances are tomorrow that file will no longer be there! As a workaround, you will add ${HOME}/bin to your PATH and place the binary there. Create a bin subdirectory in your home directory: mkdir ~/bin Copy the CLI to that subdirectory: cp ./bin/istioctl ~/bin Add your home bin subdirectory to your PATH echo \"export PATH= $HOME /bin: $PATH \" >> ~/.bashrc source ~/.bashrc Verify that istioctl is installed with: istioctl version With the CLI installed, proceed to install Istio to Kubernetes.","title":"Add istioctl to your PATH"},{"location":"install/#install-istio_1","text":"Istio can be installed directly with the cli: istioctl install When prompted, enter y to proceed to install Istio. Take a moment to learn more about Istio installation profiles .","title":"Install Istio"},{"location":"install/#verify-that-istio-is-installed","text":"List Kubernetes namespaces and note the new namespace istio-system kubectl get ns Verify that the istiod controller pod is running in that namespace kubectl get pod -n istio-system Re-run istioctl version . The output should include a control plane version, indicating that istio is indeed present in the cluster.","title":"Verify that Istio is installed"},{"location":"install/#next","text":"With Istio installed, we are ready to deploy an application to the mesh.","title":"Next"},{"location":"security/","text":"Security \u00b6","title":"Security"},{"location":"security/#security","text":"","title":"Security"},{"location":"summary/","text":"Summary \u00b6","title":"Summary"},{"location":"summary/#summary","text":"","title":"Summary"},{"location":"the-app/","text":"The application \u00b6 In this lab you will deploy an application to your mesh. The application consists of two microservers, web-frontend and customers . Info The official Istio docs canonical example is the BookInfo aplpication . For this workshop we felt that an application involving fewer microservices would be more clear. The customers service exposes a REST endpoint that returns a list of customers in JSON format. The web-frontend calls customers retrieves the list, and uses the informationt to render the customer listing in HTML. The respective docker images for these services have already been built and pushed to a docker repository. You will deploy the application to the default Kubernetes namespace. But before proceeding, we must enable sidecar injection. Enable automatic sidecar injection \u00b6 There are two options for sidecar injection : automatic and manual. In this lab we will use automatic injection, which involves labeling the namespace where the pods are to reside. Label the default namespace kubectl label namespace default istio-injection = enabled Verify that the label has been applied: kubectl get ns -Listio-injection You can list the mutating webhooks in your kubernetes cluster and confirm that the sidecar injector is present. kubectl get mutatingwebhookconfigurations If you have extra time, explore the istioctl kube-inject command. Deploy the application \u00b6 Study the two kubernetes yaml files: web-frontend.yaml and customers.yaml . web-frontend.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 --- apiVersion : v1 kind : ServiceAccount metadata : name : web-frontend --- apiVersion : apps/v1 kind : Deployment metadata : name : web-frontend labels : app : web-frontend spec : replicas : 1 selector : matchLabels : app : web-frontend template : metadata : labels : app : web-frontend version : v1 spec : serviceAccountName : web-frontend containers : - image : gcr.io/tetratelabs/web-frontend:1.0.0 imagePullPolicy : Always name : web ports : - containerPort : 8080 env : - name : CUSTOMER_SERVICE_URL value : \"http://customers.default.svc.cluster.local\" --- kind : Service apiVersion : v1 metadata : name : web-frontend labels : app : web-frontend spec : selector : app : web-frontend ports : - port : 80 name : http targetPort : 8080 customers.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 --- apiVersion : v1 kind : ServiceAccount metadata : name : customers --- apiVersion : apps/v1 kind : Deployment metadata : name : customers-v1 labels : app : customers version : v1 spec : replicas : 1 selector : matchLabels : app : customers version : v1 template : metadata : labels : app : customers version : v1 spec : serviceAccountName : customers containers : - image : gcr.io/tetratelabs/customers:1.0.0 imagePullPolicy : Always name : svc ports : - containerPort : 3000 --- kind : Service apiVersion : v1 metadata : name : customers labels : app : customers spec : selector : app : customers ports : - port : 80 name : http targetPort : 3000 Each file defines its corresponding deployment, service account, and ClusterIP service. Apply the two files to your Kubernetes cluster. kubectl apply -f customers.yaml kubectl apply -f web-frontend.yaml Confirm that: Two pods are running, one for each service Each pod consists of two containers, the one running the service image, plus the envoy sidecar kubectl get pod Verify access to each service \u00b6 Let's deploy a pod that runs a curl image so we can verify that each service is reachable from within the cluster. The istio distribution comes with a sample called sleep that will serve this purpose. Deploy sleep to the default namespace. sleep.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 # Copyright Istio Authors # # Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. ################################################################################################## # Sleep service ################################################################################################## apiVersion : v1 kind : ServiceAccount metadata : name : sleep --- apiVersion : v1 kind : Service metadata : name : sleep labels : app : sleep service : sleep spec : ports : - port : 80 name : http selector : app : sleep --- apiVersion : apps/v1 kind : Deployment metadata : name : sleep spec : replicas : 1 selector : matchLabels : app : sleep template : metadata : labels : app : sleep spec : terminationGracePeriodSeconds : 0 serviceAccountName : sleep containers : - name : sleep image : curlimages/curl command : [ \"/bin/sleep\" , \"3650d\" ] imagePullPolicy : IfNotPresent volumeMounts : - mountPath : /etc/sleep/tls name : secret-volume volumes : - name : secret-volume secret : secretName : sleep-secret optional : true --- kubectl apply -f sleep.yaml Capture the name of the sleep pod to an environment variable SLEEP_POD = $( kubectl get pod -l app = sleep -ojsonpath = '{.items[0].metadata.name}' ) Use the kubectl exec command to call the customer service. kubectl exec $SLEEP_POD -it -- curl customers The console output should show a list of customers in JSON format. Call the web-frontend service kubectl exec $SLEEP_POD -it -- curl web-frontend The console output should show an HTML page listing customers using an HTML table. Next \u00b6 In the next lab, we expose the web-frontend using an Istio Ingress Gateway. This will allow us to see this application in a web browser.","title":"The application"},{"location":"the-app/#the-application","text":"In this lab you will deploy an application to your mesh. The application consists of two microservers, web-frontend and customers . Info The official Istio docs canonical example is the BookInfo aplpication . For this workshop we felt that an application involving fewer microservices would be more clear. The customers service exposes a REST endpoint that returns a list of customers in JSON format. The web-frontend calls customers retrieves the list, and uses the informationt to render the customer listing in HTML. The respective docker images for these services have already been built and pushed to a docker repository. You will deploy the application to the default Kubernetes namespace. But before proceeding, we must enable sidecar injection.","title":"The application"},{"location":"the-app/#enable-automatic-sidecar-injection","text":"There are two options for sidecar injection : automatic and manual. In this lab we will use automatic injection, which involves labeling the namespace where the pods are to reside. Label the default namespace kubectl label namespace default istio-injection = enabled Verify that the label has been applied: kubectl get ns -Listio-injection You can list the mutating webhooks in your kubernetes cluster and confirm that the sidecar injector is present. kubectl get mutatingwebhookconfigurations If you have extra time, explore the istioctl kube-inject command.","title":"Enable automatic sidecar injection"},{"location":"the-app/#deploy-the-application","text":"Study the two kubernetes yaml files: web-frontend.yaml and customers.yaml . web-frontend.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 --- apiVersion : v1 kind : ServiceAccount metadata : name : web-frontend --- apiVersion : apps/v1 kind : Deployment metadata : name : web-frontend labels : app : web-frontend spec : replicas : 1 selector : matchLabels : app : web-frontend template : metadata : labels : app : web-frontend version : v1 spec : serviceAccountName : web-frontend containers : - image : gcr.io/tetratelabs/web-frontend:1.0.0 imagePullPolicy : Always name : web ports : - containerPort : 8080 env : - name : CUSTOMER_SERVICE_URL value : \"http://customers.default.svc.cluster.local\" --- kind : Service apiVersion : v1 metadata : name : web-frontend labels : app : web-frontend spec : selector : app : web-frontend ports : - port : 80 name : http targetPort : 8080 customers.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 --- apiVersion : v1 kind : ServiceAccount metadata : name : customers --- apiVersion : apps/v1 kind : Deployment metadata : name : customers-v1 labels : app : customers version : v1 spec : replicas : 1 selector : matchLabels : app : customers version : v1 template : metadata : labels : app : customers version : v1 spec : serviceAccountName : customers containers : - image : gcr.io/tetratelabs/customers:1.0.0 imagePullPolicy : Always name : svc ports : - containerPort : 3000 --- kind : Service apiVersion : v1 metadata : name : customers labels : app : customers spec : selector : app : customers ports : - port : 80 name : http targetPort : 3000 Each file defines its corresponding deployment, service account, and ClusterIP service. Apply the two files to your Kubernetes cluster. kubectl apply -f customers.yaml kubectl apply -f web-frontend.yaml Confirm that: Two pods are running, one for each service Each pod consists of two containers, the one running the service image, plus the envoy sidecar kubectl get pod","title":"Deploy the application"},{"location":"the-app/#verify-access-to-each-service","text":"Let's deploy a pod that runs a curl image so we can verify that each service is reachable from within the cluster. The istio distribution comes with a sample called sleep that will serve this purpose. Deploy sleep to the default namespace. sleep.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 # Copyright Istio Authors # # Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. ################################################################################################## # Sleep service ################################################################################################## apiVersion : v1 kind : ServiceAccount metadata : name : sleep --- apiVersion : v1 kind : Service metadata : name : sleep labels : app : sleep service : sleep spec : ports : - port : 80 name : http selector : app : sleep --- apiVersion : apps/v1 kind : Deployment metadata : name : sleep spec : replicas : 1 selector : matchLabels : app : sleep template : metadata : labels : app : sleep spec : terminationGracePeriodSeconds : 0 serviceAccountName : sleep containers : - name : sleep image : curlimages/curl command : [ \"/bin/sleep\" , \"3650d\" ] imagePullPolicy : IfNotPresent volumeMounts : - mountPath : /etc/sleep/tls name : secret-volume volumes : - name : secret-volume secret : secretName : sleep-secret optional : true --- kubectl apply -f sleep.yaml Capture the name of the sleep pod to an environment variable SLEEP_POD = $( kubectl get pod -l app = sleep -ojsonpath = '{.items[0].metadata.name}' ) Use the kubectl exec command to call the customer service. kubectl exec $SLEEP_POD -it -- curl customers The console output should show a list of customers in JSON format. Call the web-frontend service kubectl exec $SLEEP_POD -it -- curl web-frontend The console output should show an HTML page listing customers using an HTML table.","title":"Verify access to each service"},{"location":"the-app/#next","text":"In the next lab, we expose the web-frontend using an Istio Ingress Gateway. This will allow us to see this application in a web browser.","title":"Next"},{"location":"traffic-shifting/","text":"Traffic shifting \u00b6","title":"Traffic shifting"},{"location":"traffic-shifting/#traffic-shifting","text":"","title":"Traffic shifting"}]}