{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome! \u00b6 Welcome to the Istio 0 to 60 workshop! On this site you will find the hands-on labs for the workshop. In the first lab, we walk you through accessing and configuring your lab environment . Let's begin.","title":"Welcome!"},{"location":"#welcome","text":"Welcome to the Istio 0 to 60 workshop! On this site you will find the hands-on labs for the workshop. In the first lab, we walk you through accessing and configuring your lab environment . Let's begin.","title":"Welcome!"},{"location":"dashboards/","text":"Observability \u00b6 In this lab we explore one of the main strengths of Istio: observability. The services in our mesh are automatically observable, without adding any burden on developers. Deploy the Addons \u00b6 The istio distribution provides addons for a number of systems that together provide observability for the service mesh: Zipkin or Jaeger for distributed tracing Prometheus for metrics collection Grafana provides dashboards for monitoring, using Prometheus as the data source Kiali allows us to visualize the mesh These addons are located in the samples/addons/ folder of the distribution. Navigate to the addons directory cd ~/istio-1.12.2/samples/addons Deploy each addon: kubectl apply -f extras/zipkin.yaml kubectl apply -f prometheus.yaml kubectl apply -f grafana.yaml kubectl apply -f kiali.yaml Verify that the istio-system namespace is now running additional workloads for each of the addons. kubectl get pod -n istio-system The istioctl CLI provides convenience commands for accessing the web UIs for each dashboard. Take a moment to review the help output for the istioctl dashboard command: istioctl dashboard --help Generate a load \u00b6 In order to have something to observe, we need to generate a load on our system. Install a load generator \u00b6 Install a simple load generating tool named siege . [tbd] Familiarize yourself with the command and its options. siege --help Run the following command to generate a mild load against the application. siege --delay = 3 --concurrent = 3 --time = 20M http:// $GATEWAY_IP / Note The siege command stays in the foreground while it runs. It may be simplest to leave it running, and open a separate terminal in your cloud shell environment. Kiali \u00b6 Launch the Kiali dashboard: istioctl dashboard kiali Note The istioctl dashboard command also blocks. Leave it running until you're finished using the dashboard, at which time pressing Ctrl + C can interrupt the process and put you back at the terminal prompt. The Kiali dashboard displays. Customize the view as follows: Select the Graph section from the sidebar. From the Namespace \"pulldown\" menu at the top of the screen, select the default namespace, the location where the application's pods are running. From the third pulldown menu, select App graph . From the \"Display\" pulldown , toggle on Traffic Animation and Security . From the footer, toggle the legend so that it is visible. Take a moment to familiarize yourself with the legend. Observe the visualization and note the following: We can see traffic coming in through the ingress gateway to the web-frontend , and the subsequent calls from the web-frontend to the customers service The lines connecting the services are green, indicating healthy requests The small lock icon on each edge in the graph indicates that the traffic is secured with mutual TLS Such visualizations are helpful in understanding the flow of requests in the mesh and in diagnosis. Feel free to spend more time exploring Kiali. We will revisit Kiali in a later lab to visualize traffic shifting such as when performing a blue-green or canary deployment. Kiali Cleanup \u00b6 Close the Kiali dashboard. Interrupt the istioctl dashboard kiali command by pressing Ctrl + C . Zipkin \u00b6 Launch the Zipkin dashboard: istioctl dashboard zipkin The zipkin dashboard displays. Click on the red '+' button and select \"serviceName\". Select the service named web-frontend.default and click on the Run Query button (lightblue) to the right. A number of query results will display. Each row is expandable and will display more detail in terms of the services participating in that particular trace. Click the Show button to the right of one of the traces. The resulting view shows spans that are part of the trace, and more importantly how much time was spent within each span. Such information can help diagnose slow requests and pin-point where the latency lies. Such traces also help us make sense of the flow of requests in a microservice architecture. Zipkin Cleanup \u00b6 Close the Zipking dashboard. Interrupt the istioctl dashboard zipkin command with Ctrl + C . Prometheus \u00b6 Prometheus works by periodically calling a metrics endpoint against each running service, this endpoint is usually termed the \"scrape\" endpoint. Developers normally have to instrument their applications to expose such an endpoint and return metrics information in the format the Prometheus expects. With Istio, this is done automatically by envoy. Observe how Envoy exposes a Prometheus scrape endpoint \u00b6 Capture the customers pod name to a variable. CUSTOMERS_POD = $( kubectl get pod -l app = customers -ojsonpath = '{.items[0].metadata.name}' ) Run the following command: kubectl exec $CUSTOMERS_POD -it -- curl localhost:15090/stats/prometheus | grep istio_requests The list of metrics returned by the endpoint is rather lengthy, so we just peek at the \"istio_requests\" metric. But the full response contains many more metrics. Access the dashboard \u00b6 Start the prometheus dashboard istioctl dashboard prometheus In the search field enter the metric named istio_requests_total , and click the Execute button (on the right). Select the tab named \"Graph\" to obtain a graphical representation of this metric over time. Note that you are looking it requests across the entire mesh, i.e. this includes both requests to web-frontend and to customers . As an example of Prometheus' dimensional metrics capability, we can ask for total requests having a response code of 200: istio_requests_total{response_code=\"200\"} With respects to requests, it's more interesting to look at the rate of incoming requests over a time window. Try: rate(istio_requests_total[5m]) There's much more to the Prometheus query language ( this may be a good place to start). Grafana consumes these metrics to produce graphs on our behalf. Grafana \u00b6 Launch the Grafana dashboard istioctl dashboard grafana From the sidebar, select Dashboards -> Manager Click on the folder named Istio to reveal pre-designed Istio-specific Grafana dashboards Explore the Istio Mesh Dashboard. Note the Global Request Volume and Global Success Rate. Explore the Istio Service Dashboard. First select the service web-frontend and inspect its metrics, then switch to the customers service and review its dashboard. Explore the Istio Workload Dashboard. Select the web-frontend workload. Look at Outbound Services and note the outbound requests to the customers service. Select the customres workload and note that it makes no Oubtound Services calls. Feel free to further explore these dashboards. Cleanup \u00b6 Terminate the istioctl dashboard command ( Ctrl + C ) Likewise, terminate the siege command Next \u00b6 We turn our attention next to security features of a service mesh.","title":"Observability"},{"location":"dashboards/#observability","text":"In this lab we explore one of the main strengths of Istio: observability. The services in our mesh are automatically observable, without adding any burden on developers.","title":"Observability"},{"location":"dashboards/#deploy-the-addons","text":"The istio distribution provides addons for a number of systems that together provide observability for the service mesh: Zipkin or Jaeger for distributed tracing Prometheus for metrics collection Grafana provides dashboards for monitoring, using Prometheus as the data source Kiali allows us to visualize the mesh These addons are located in the samples/addons/ folder of the distribution. Navigate to the addons directory cd ~/istio-1.12.2/samples/addons Deploy each addon: kubectl apply -f extras/zipkin.yaml kubectl apply -f prometheus.yaml kubectl apply -f grafana.yaml kubectl apply -f kiali.yaml Verify that the istio-system namespace is now running additional workloads for each of the addons. kubectl get pod -n istio-system The istioctl CLI provides convenience commands for accessing the web UIs for each dashboard. Take a moment to review the help output for the istioctl dashboard command: istioctl dashboard --help","title":"Deploy the Addons"},{"location":"dashboards/#generate-a-load","text":"In order to have something to observe, we need to generate a load on our system.","title":"Generate a load"},{"location":"dashboards/#install-a-load-generator","text":"Install a simple load generating tool named siege . [tbd] Familiarize yourself with the command and its options. siege --help Run the following command to generate a mild load against the application. siege --delay = 3 --concurrent = 3 --time = 20M http:// $GATEWAY_IP / Note The siege command stays in the foreground while it runs. It may be simplest to leave it running, and open a separate terminal in your cloud shell environment.","title":"Install a load generator"},{"location":"dashboards/#kiali","text":"Launch the Kiali dashboard: istioctl dashboard kiali Note The istioctl dashboard command also blocks. Leave it running until you're finished using the dashboard, at which time pressing Ctrl + C can interrupt the process and put you back at the terminal prompt. The Kiali dashboard displays. Customize the view as follows: Select the Graph section from the sidebar. From the Namespace \"pulldown\" menu at the top of the screen, select the default namespace, the location where the application's pods are running. From the third pulldown menu, select App graph . From the \"Display\" pulldown , toggle on Traffic Animation and Security . From the footer, toggle the legend so that it is visible. Take a moment to familiarize yourself with the legend. Observe the visualization and note the following: We can see traffic coming in through the ingress gateway to the web-frontend , and the subsequent calls from the web-frontend to the customers service The lines connecting the services are green, indicating healthy requests The small lock icon on each edge in the graph indicates that the traffic is secured with mutual TLS Such visualizations are helpful in understanding the flow of requests in the mesh and in diagnosis. Feel free to spend more time exploring Kiali. We will revisit Kiali in a later lab to visualize traffic shifting such as when performing a blue-green or canary deployment.","title":"Kiali"},{"location":"dashboards/#kiali-cleanup","text":"Close the Kiali dashboard. Interrupt the istioctl dashboard kiali command by pressing Ctrl + C .","title":"Kiali Cleanup"},{"location":"dashboards/#zipkin","text":"Launch the Zipkin dashboard: istioctl dashboard zipkin The zipkin dashboard displays. Click on the red '+' button and select \"serviceName\". Select the service named web-frontend.default and click on the Run Query button (lightblue) to the right. A number of query results will display. Each row is expandable and will display more detail in terms of the services participating in that particular trace. Click the Show button to the right of one of the traces. The resulting view shows spans that are part of the trace, and more importantly how much time was spent within each span. Such information can help diagnose slow requests and pin-point where the latency lies. Such traces also help us make sense of the flow of requests in a microservice architecture.","title":"Zipkin"},{"location":"dashboards/#zipkin-cleanup","text":"Close the Zipking dashboard. Interrupt the istioctl dashboard zipkin command with Ctrl + C .","title":"Zipkin Cleanup"},{"location":"dashboards/#prometheus","text":"Prometheus works by periodically calling a metrics endpoint against each running service, this endpoint is usually termed the \"scrape\" endpoint. Developers normally have to instrument their applications to expose such an endpoint and return metrics information in the format the Prometheus expects. With Istio, this is done automatically by envoy.","title":"Prometheus"},{"location":"dashboards/#observe-how-envoy-exposes-a-prometheus-scrape-endpoint","text":"Capture the customers pod name to a variable. CUSTOMERS_POD = $( kubectl get pod -l app = customers -ojsonpath = '{.items[0].metadata.name}' ) Run the following command: kubectl exec $CUSTOMERS_POD -it -- curl localhost:15090/stats/prometheus | grep istio_requests The list of metrics returned by the endpoint is rather lengthy, so we just peek at the \"istio_requests\" metric. But the full response contains many more metrics.","title":"Observe how Envoy exposes a Prometheus scrape endpoint"},{"location":"dashboards/#access-the-dashboard","text":"Start the prometheus dashboard istioctl dashboard prometheus In the search field enter the metric named istio_requests_total , and click the Execute button (on the right). Select the tab named \"Graph\" to obtain a graphical representation of this metric over time. Note that you are looking it requests across the entire mesh, i.e. this includes both requests to web-frontend and to customers . As an example of Prometheus' dimensional metrics capability, we can ask for total requests having a response code of 200: istio_requests_total{response_code=\"200\"} With respects to requests, it's more interesting to look at the rate of incoming requests over a time window. Try: rate(istio_requests_total[5m]) There's much more to the Prometheus query language ( this may be a good place to start). Grafana consumes these metrics to produce graphs on our behalf.","title":"Access the dashboard"},{"location":"dashboards/#grafana","text":"Launch the Grafana dashboard istioctl dashboard grafana From the sidebar, select Dashboards -> Manager Click on the folder named Istio to reveal pre-designed Istio-specific Grafana dashboards Explore the Istio Mesh Dashboard. Note the Global Request Volume and Global Success Rate. Explore the Istio Service Dashboard. First select the service web-frontend and inspect its metrics, then switch to the customers service and review its dashboard. Explore the Istio Workload Dashboard. Select the web-frontend workload. Look at Outbound Services and note the outbound requests to the customers service. Select the customres workload and note that it makes no Oubtound Services calls. Feel free to further explore these dashboards.","title":"Grafana"},{"location":"dashboards/#cleanup","text":"Terminate the istioctl dashboard command ( Ctrl + C ) Likewise, terminate the siege command","title":"Cleanup"},{"location":"dashboards/#next","text":"We turn our attention next to security features of a service mesh.","title":"Next"},{"location":"environment/","text":"Lab environment \u00b6 An environment has been provisioned for you on Google Cloud Platform (GCP), consisting mainly of a Kubernetes cluster using Google Kubernetes Engine (GKE). Log in to GCP \u00b6 Log in to GCP using credentials provided by your instructor. [instructions tbd] Select your project \u00b6 Select the project you have been assigned. [instructions tbd] Launch the Cloud Shell \u00b6 The Google Cloud Shell will serve as your terminal environment for these labs. [instructions tbd] Verify cluster access \u00b6 Check that kubectl is installed kubectl version Verify that your kubernetes context is set to your cluster kubectl config get-contexts List namespaces kubectl get ns Artifacts \u00b6 The lab instructions reference Kubernetes yaml artifacts that you will need to apply to your cluster at specific points in time. You have the option of copying and pasting the yaml directly from the lab instructions. Another, perhaps simpler method is to clone the GitHub repository for this workshop to your Google Cloud Shell environment. You will find all yaml artifacts in the subdirectory named artifacts . git clone https://github.com/tetratelabs/istio-0to60.git Next \u00b6 Now that we have access to our environment and to our Kubernetes cluster, we can proceed to install Istio.","title":"Lab environment"},{"location":"environment/#lab-environment","text":"An environment has been provisioned for you on Google Cloud Platform (GCP), consisting mainly of a Kubernetes cluster using Google Kubernetes Engine (GKE).","title":"Lab environment"},{"location":"environment/#log-in-to-gcp","text":"Log in to GCP using credentials provided by your instructor. [instructions tbd]","title":"Log in to GCP"},{"location":"environment/#select-your-project","text":"Select the project you have been assigned. [instructions tbd]","title":"Select your project"},{"location":"environment/#launch-the-cloud-shell","text":"The Google Cloud Shell will serve as your terminal environment for these labs. [instructions tbd]","title":"Launch the Cloud Shell"},{"location":"environment/#verify-cluster-access","text":"Check that kubectl is installed kubectl version Verify that your kubernetes context is set to your cluster kubectl config get-contexts List namespaces kubectl get ns","title":"Verify cluster access"},{"location":"environment/#artifacts","text":"The lab instructions reference Kubernetes yaml artifacts that you will need to apply to your cluster at specific points in time. You have the option of copying and pasting the yaml directly from the lab instructions. Another, perhaps simpler method is to clone the GitHub repository for this workshop to your Google Cloud Shell environment. You will find all yaml artifacts in the subdirectory named artifacts . git clone https://github.com/tetratelabs/istio-0to60.git","title":"Artifacts"},{"location":"environment/#next","text":"Now that we have access to our environment and to our Kubernetes cluster, we can proceed to install Istio.","title":"Next"},{"location":"ingress/","text":"Ingress \u00b6 The main objective of this lab is to expose the web frontend to the public internet. The ingress gateway \u00b6 When you installed Istio, in addition to deploying istiod to Kubernetes, the installation also provisioned an Ingress Gateway. View the corresponding istio ingress gateway pod in the istio-system namespace. kubectl get pod -n istio-system A corresponding LoadBalancer type service was also created: kubectl get svc -n istio-system Make a note of the external IP address for the load balancer. Assign it to an environment variable. GATEWAY_IP = $( kubectl get svc -n istio-system istio-ingressgateway -ojsonpath = '{.status.loadBalancer.ingress[0].ip}' ) We could create a DNS A record for this IP address, but for the sake of simplicity, we will access anything we expose from the mesh by using the IP address directly. Configuring ingress \u00b6 Configuring ingress with Istio is split into two parts: Define a Gateway custom resource that governs the specific host, port, and protocol to expose Specify how requests should be routed with a VirtualService custom resource. Create a Gateway resource \u00b6 Review the following Gateway specification. gateway.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 --- apiVersion : networking.istio.io/v1alpha3 kind : Gateway metadata : name : frontend-gateway spec : selector : istio : ingressgateway servers : - port : number : 80 name : http protocol : HTTP hosts : - \"*\" We specify the HTTP protocol, port 80, and the wildcard host value ensures a match against HTTP requests that reference the load balancer IP address $GATEWAY_IP . The selector istio: ingressgateway ensures that this gateway resource binds to the physical ingress gateway. Apply the gateway resource to your cluster. kubectl apply -f gateway.yaml Attempt an HTTP request in your browser to the gateway IP address. It should return a 404 (not found). Create a VirtualService resource \u00b6 Review the following VirtualService specification. web-frontend-virtualservice.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 --- apiVersion : networking.istio.io/v1alpha3 kind : VirtualService metadata : name : web-frontend spec : hosts : - \"*\" gateways : - frontend-gateway http : - route : - destination : host : web-frontend.default.svc.cluster.local port : number : 80 Note how this specification references the name of the gateway (\"frontend-gateway\"), a matching host (\"*\"), and specifies a route for requests to be directed to the web-frontend service. Apply the virtual service resource to your cluster. kubectl apply -f web-frontend-virtualservice.yaml List virtual services in the default namespace. kubectl get virtualservice The output indicates that the virtual service named web-frontend is bound to the gateway, as well as any hostname that routes to the load balancer IP address. Finally, verify that you can now access web-frontend from your web browser using the gateway IP address. Candidate follow-on exercises \u00b6 Consider creating a DNS A record for the gateway IP, and narrowing down the scope of the gateway to only match that hostname. Configuring a TLS ingress gateway Next \u00b6 The application is now running and exposed on the internet. In the next lab, we turn our attention to the observability features that are built in to Istio.","title":"Ingress"},{"location":"ingress/#ingress","text":"The main objective of this lab is to expose the web frontend to the public internet.","title":"Ingress"},{"location":"ingress/#the-ingress-gateway","text":"When you installed Istio, in addition to deploying istiod to Kubernetes, the installation also provisioned an Ingress Gateway. View the corresponding istio ingress gateway pod in the istio-system namespace. kubectl get pod -n istio-system A corresponding LoadBalancer type service was also created: kubectl get svc -n istio-system Make a note of the external IP address for the load balancer. Assign it to an environment variable. GATEWAY_IP = $( kubectl get svc -n istio-system istio-ingressgateway -ojsonpath = '{.status.loadBalancer.ingress[0].ip}' ) We could create a DNS A record for this IP address, but for the sake of simplicity, we will access anything we expose from the mesh by using the IP address directly.","title":"The ingress gateway"},{"location":"ingress/#configuring-ingress","text":"Configuring ingress with Istio is split into two parts: Define a Gateway custom resource that governs the specific host, port, and protocol to expose Specify how requests should be routed with a VirtualService custom resource.","title":"Configuring ingress"},{"location":"ingress/#create-a-gateway-resource","text":"Review the following Gateway specification. gateway.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 --- apiVersion : networking.istio.io/v1alpha3 kind : Gateway metadata : name : frontend-gateway spec : selector : istio : ingressgateway servers : - port : number : 80 name : http protocol : HTTP hosts : - \"*\" We specify the HTTP protocol, port 80, and the wildcard host value ensures a match against HTTP requests that reference the load balancer IP address $GATEWAY_IP . The selector istio: ingressgateway ensures that this gateway resource binds to the physical ingress gateway. Apply the gateway resource to your cluster. kubectl apply -f gateway.yaml Attempt an HTTP request in your browser to the gateway IP address. It should return a 404 (not found).","title":"Create a Gateway resource"},{"location":"ingress/#create-a-virtualservice-resource","text":"Review the following VirtualService specification. web-frontend-virtualservice.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 --- apiVersion : networking.istio.io/v1alpha3 kind : VirtualService metadata : name : web-frontend spec : hosts : - \"*\" gateways : - frontend-gateway http : - route : - destination : host : web-frontend.default.svc.cluster.local port : number : 80 Note how this specification references the name of the gateway (\"frontend-gateway\"), a matching host (\"*\"), and specifies a route for requests to be directed to the web-frontend service. Apply the virtual service resource to your cluster. kubectl apply -f web-frontend-virtualservice.yaml List virtual services in the default namespace. kubectl get virtualservice The output indicates that the virtual service named web-frontend is bound to the gateway, as well as any hostname that routes to the load balancer IP address. Finally, verify that you can now access web-frontend from your web browser using the gateway IP address.","title":"Create a VirtualService resource"},{"location":"ingress/#candidate-follow-on-exercises","text":"Consider creating a DNS A record for the gateway IP, and narrowing down the scope of the gateway to only match that hostname. Configuring a TLS ingress gateway","title":"Candidate follow-on exercises"},{"location":"ingress/#next","text":"The application is now running and exposed on the internet. In the next lab, we turn our attention to the observability features that are built in to Istio.","title":"Next"},{"location":"install/","text":"Install Istio \u00b6 In this lab you will install Istio. Download Istio \u00b6 Run the following command from your home directory. curl -L https://istio.io/downloadIstio | sh - Navigate into the directory created by the above command. cd istio-1.12.2 Add istioctl to your PATH \u00b6 The istioctl CLI is located in the bin/ subdirectory. Note Cloud Shell only preserves files located inside your home directory across sessions. This means that if you install a binary to a PATH such as /usr/local/bin , chances are tomorrow that file will no longer be there! As a workaround, you will add ${HOME}/bin to your PATH and place the binary there. Create a bin subdirectory in your home directory: mkdir ~/bin Copy the CLI to that subdirectory: cp ./bin/istioctl ~/bin Add your home bin subdirectory to your PATH echo \"export PATH= $HOME /bin: $PATH \" >> ~/.bashrc source ~/.bashrc Verify that istioctl is installed with: istioctl version With the CLI installed, proceed to install Istio to Kubernetes. Install Istio \u00b6 Istio can be installed directly with the cli: istioctl install When prompted, enter y to proceed to install Istio. Take a moment to learn more about Istio installation profiles . Verify that Istio is installed \u00b6 List Kubernetes namespaces and note the new namespace istio-system kubectl get ns Verify that the istiod controller pod is running in that namespace kubectl get pod -n istio-system Re-run istioctl version . The output should include a control plane version, indicating that istio is indeed present in the cluster. Next \u00b6 With Istio installed, we are ready to deploy an application to the mesh.","title":"Install Istio"},{"location":"install/#install-istio","text":"In this lab you will install Istio.","title":"Install Istio"},{"location":"install/#download-istio","text":"Run the following command from your home directory. curl -L https://istio.io/downloadIstio | sh - Navigate into the directory created by the above command. cd istio-1.12.2","title":"Download Istio"},{"location":"install/#add-istioctl-to-your-path","text":"The istioctl CLI is located in the bin/ subdirectory. Note Cloud Shell only preserves files located inside your home directory across sessions. This means that if you install a binary to a PATH such as /usr/local/bin , chances are tomorrow that file will no longer be there! As a workaround, you will add ${HOME}/bin to your PATH and place the binary there. Create a bin subdirectory in your home directory: mkdir ~/bin Copy the CLI to that subdirectory: cp ./bin/istioctl ~/bin Add your home bin subdirectory to your PATH echo \"export PATH= $HOME /bin: $PATH \" >> ~/.bashrc source ~/.bashrc Verify that istioctl is installed with: istioctl version With the CLI installed, proceed to install Istio to Kubernetes.","title":"Add istioctl to your PATH"},{"location":"install/#install-istio_1","text":"Istio can be installed directly with the cli: istioctl install When prompted, enter y to proceed to install Istio. Take a moment to learn more about Istio installation profiles .","title":"Install Istio"},{"location":"install/#verify-that-istio-is-installed","text":"List Kubernetes namespaces and note the new namespace istio-system kubectl get ns Verify that the istiod controller pod is running in that namespace kubectl get pod -n istio-system Re-run istioctl version . The output should include a control plane version, indicating that istio is indeed present in the cluster.","title":"Verify that Istio is installed"},{"location":"install/#next","text":"With Istio installed, we are ready to deploy an application to the mesh.","title":"Next"},{"location":"security/","text":"Security \u00b6 In this lab we explore some of the security features of the Istio service mesh. Mutual TLS \u00b6 By default, Istio is configured such that when a service is deployed onto the mesh, it will take advantage of mutual TLS: the service is given an identity as a function of its associated service account and namespace an x.509 certificate is issued to the workload (and regularly rotated) and used to identify the workload in calls to other services In the observability lab, we looked at the Kiali dashboard and noted the lock icons indicating that traffic was secured with mTLS. Can a workload receive plain-text requests? \u00b6 We can test whether a mesh workload, such as the customers service, will allow a plain-text request as follows: Create a separate namespace that is not configured with automatic injection. kubectl create ns otherns Deploy sleep to that namespace kubectl apply -f sleep.yaml -n otherns Verify that the sleep pod has no sidecars: kubectl get pod -n otherns Call the customer service from that pod: SLEEP_POD = $( kubectl get pod -l app = sleep -n otherns -ojsonpath = '{.items[0].metadata.name}' ) kubectl exec -n otherns $SLEEP_POD -it -- curl customers.default The output should look like a list of customers in JSON format. We conclude that Istio is configured by default to allow plain-text request. This is called permissive mode and is specifically designed to allow services that have not yet fully onboarded onto the mesh to participate. Enable strict mode \u00b6 Istio provides the PeerAuthentication custom resource to define peer authenticaion policy. Apply the following peer authentication policy. mtls-strict.yaml 1 2 3 4 5 6 7 8 9 --- apiVersion : security.istio.io/v1beta1 kind : PeerAuthentication metadata : name : default namespace : default spec : mtls : mode : STRICT Tip Strict mtls can be enabled globally by setting the namespace to the name of the istio root namespace, which by default is istio-system Verify that the peer authentication has been applied. kubectl get peerauthentication Verify that plain-text requests are no longer permitted \u00b6 kubectl exec -n otherns $SLEEP_POD -it -- curl customers.default The console output should indicate that the connection was reset by peer. Security in depth \u00b6 Another important layer of security is to define an authorization policy, in which we allow only specific services to communicate with other services. At the moment, any container can, for example, call the customers service or the web-frontend service. Capture the name of the sleep pod runnin in the default namespace SLEEP_POD = $( kubectl get pod -l app = sleep -ojsonpath = '{.items[0].metadata.name}' ) Call the customers service. kubectl exec $SLEEP_POD -it -- curl customers Call the web-frontend service. kubectl exec $SLEEP_POD -it -- curl web-frontend Both calls succeed. Let's to apply a policy in which only web-frontend is allowed to call customers , and only the ingress gateway can call web-frontend . Study the below authorization policy. authn-policy-customers.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 --- apiVersion : security.istio.io/v1beta1 kind : AuthorizationPolicy metadata : name : allowed-customers-clients namespace : default spec : selector : matchLabels : app : customers action : ALLOW rules : - from : - source : principals : [ \"cluster.local/ns/default/sa/web-frontend\" ] The selector section specifies that the policy applies to the customers service. Note how the rules have a \"from: source: \" section indicating who is allowed in. The nomenclature for the value of the principals field comes from the spiffe standard. Note how it captures the service account name and namespace associated with the web-frontend service. This identify is associated with the x.509 certificate used by each service when making secure mtls calls to one another. Challenge \u00b6 Can you come up with a similar authorization policy for web-frontend ? Use the customers authorization policy as a template Revise the selector to match the web-frontend service Revise the rule to match the principal for the ingress gateway Hint The ingress gateway has its own identify. Here is a command which can help you find the name of the service account associated with its identify: kubectl get pod -n istio-system -l app = istio-ingressgateway -o yaml | grep serviceAccountName Use it together with the namespace that the ingress gateway is running in to specify the value for the principals field. Test it \u00b6 Don't forget to verify that the policy is enforced. Call both services again from the sleep pod and ensure communication is no longer allowed. The console output should contain the message RBAC: access denied . Next \u00b6 In the next lab we show how to use Istio's traffic management features to upgrade the customers service with zero downtime.","title":"Security"},{"location":"security/#security","text":"In this lab we explore some of the security features of the Istio service mesh.","title":"Security"},{"location":"security/#mutual-tls","text":"By default, Istio is configured such that when a service is deployed onto the mesh, it will take advantage of mutual TLS: the service is given an identity as a function of its associated service account and namespace an x.509 certificate is issued to the workload (and regularly rotated) and used to identify the workload in calls to other services In the observability lab, we looked at the Kiali dashboard and noted the lock icons indicating that traffic was secured with mTLS.","title":"Mutual TLS"},{"location":"security/#can-a-workload-receive-plain-text-requests","text":"We can test whether a mesh workload, such as the customers service, will allow a plain-text request as follows: Create a separate namespace that is not configured with automatic injection. kubectl create ns otherns Deploy sleep to that namespace kubectl apply -f sleep.yaml -n otherns Verify that the sleep pod has no sidecars: kubectl get pod -n otherns Call the customer service from that pod: SLEEP_POD = $( kubectl get pod -l app = sleep -n otherns -ojsonpath = '{.items[0].metadata.name}' ) kubectl exec -n otherns $SLEEP_POD -it -- curl customers.default The output should look like a list of customers in JSON format. We conclude that Istio is configured by default to allow plain-text request. This is called permissive mode and is specifically designed to allow services that have not yet fully onboarded onto the mesh to participate.","title":"Can a workload receive plain-text requests?"},{"location":"security/#enable-strict-mode","text":"Istio provides the PeerAuthentication custom resource to define peer authenticaion policy. Apply the following peer authentication policy. mtls-strict.yaml 1 2 3 4 5 6 7 8 9 --- apiVersion : security.istio.io/v1beta1 kind : PeerAuthentication metadata : name : default namespace : default spec : mtls : mode : STRICT Tip Strict mtls can be enabled globally by setting the namespace to the name of the istio root namespace, which by default is istio-system Verify that the peer authentication has been applied. kubectl get peerauthentication","title":"Enable strict mode"},{"location":"security/#verify-that-plain-text-requests-are-no-longer-permitted","text":"kubectl exec -n otherns $SLEEP_POD -it -- curl customers.default The console output should indicate that the connection was reset by peer.","title":"Verify that plain-text requests are no longer permitted"},{"location":"security/#security-in-depth","text":"Another important layer of security is to define an authorization policy, in which we allow only specific services to communicate with other services. At the moment, any container can, for example, call the customers service or the web-frontend service. Capture the name of the sleep pod runnin in the default namespace SLEEP_POD = $( kubectl get pod -l app = sleep -ojsonpath = '{.items[0].metadata.name}' ) Call the customers service. kubectl exec $SLEEP_POD -it -- curl customers Call the web-frontend service. kubectl exec $SLEEP_POD -it -- curl web-frontend Both calls succeed. Let's to apply a policy in which only web-frontend is allowed to call customers , and only the ingress gateway can call web-frontend . Study the below authorization policy. authn-policy-customers.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 --- apiVersion : security.istio.io/v1beta1 kind : AuthorizationPolicy metadata : name : allowed-customers-clients namespace : default spec : selector : matchLabels : app : customers action : ALLOW rules : - from : - source : principals : [ \"cluster.local/ns/default/sa/web-frontend\" ] The selector section specifies that the policy applies to the customers service. Note how the rules have a \"from: source: \" section indicating who is allowed in. The nomenclature for the value of the principals field comes from the spiffe standard. Note how it captures the service account name and namespace associated with the web-frontend service. This identify is associated with the x.509 certificate used by each service when making secure mtls calls to one another.","title":"Security in depth"},{"location":"security/#challenge","text":"Can you come up with a similar authorization policy for web-frontend ? Use the customers authorization policy as a template Revise the selector to match the web-frontend service Revise the rule to match the principal for the ingress gateway Hint The ingress gateway has its own identify. Here is a command which can help you find the name of the service account associated with its identify: kubectl get pod -n istio-system -l app = istio-ingressgateway -o yaml | grep serviceAccountName Use it together with the namespace that the ingress gateway is running in to specify the value for the principals field.","title":"Challenge"},{"location":"security/#test-it","text":"Don't forget to verify that the policy is enforced. Call both services again from the sleep pod and ensure communication is no longer allowed. The console output should contain the message RBAC: access denied .","title":"Test it"},{"location":"security/#next","text":"In the next lab we show how to use Istio's traffic management features to upgrade the customers service with zero downtime.","title":"Next"},{"location":"summary/","text":"Summary \u00b6","title":"Summary"},{"location":"summary/#summary","text":"","title":"Summary"},{"location":"the-app/","text":"The application \u00b6 In this lab you will deploy an application to your mesh. The application consists of two microservers, web-frontend and customers . Info The official Istio docs canonical example is the BookInfo aplpication . For this workshop we felt that an application involving fewer microservices would be more clear. The customers service exposes a REST endpoint that returns a list of customers in JSON format. The web-frontend calls customers retrieves the list, and uses the informationt to render the customer listing in HTML. The respective docker images for these services have already been built and pushed to a docker repository. You will deploy the application to the default Kubernetes namespace. But before proceeding, we must enable sidecar injection. Enable automatic sidecar injection \u00b6 There are two options for sidecar injection : automatic and manual. In this lab we will use automatic injection, which involves labeling the namespace where the pods are to reside. Label the default namespace kubectl label namespace default istio-injection = enabled Verify that the label has been applied: kubectl get ns -Listio-injection You can list the mutating webhooks in your kubernetes cluster and confirm that the sidecar injector is present. kubectl get mutatingwebhookconfigurations If you have extra time, explore the istioctl kube-inject command. Deploy the application \u00b6 Study the two kubernetes yaml files: web-frontend.yaml and customers.yaml . web-frontend.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 --- apiVersion : v1 kind : ServiceAccount metadata : name : web-frontend --- apiVersion : apps/v1 kind : Deployment metadata : name : web-frontend labels : app : web-frontend spec : replicas : 1 selector : matchLabels : app : web-frontend template : metadata : labels : app : web-frontend version : v1 spec : serviceAccountName : web-frontend containers : - image : gcr.io/tetratelabs/web-frontend:1.0.0 imagePullPolicy : Always name : web ports : - containerPort : 8080 env : - name : CUSTOMER_SERVICE_URL value : \"http://customers.default.svc.cluster.local\" --- kind : Service apiVersion : v1 metadata : name : web-frontend labels : app : web-frontend spec : selector : app : web-frontend ports : - port : 80 name : http targetPort : 8080 customers.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 --- apiVersion : v1 kind : ServiceAccount metadata : name : customers --- apiVersion : apps/v1 kind : Deployment metadata : name : customers-v1 labels : app : customers version : v1 spec : replicas : 1 selector : matchLabels : app : customers version : v1 template : metadata : labels : app : customers version : v1 spec : serviceAccountName : customers containers : - image : gcr.io/tetratelabs/customers:1.0.0 imagePullPolicy : Always name : svc ports : - containerPort : 3000 --- kind : Service apiVersion : v1 metadata : name : customers labels : app : customers spec : selector : app : customers ports : - port : 80 name : http targetPort : 3000 Each file defines its corresponding deployment, service account, and ClusterIP service. Apply the two files to your Kubernetes cluster. kubectl apply -f customers.yaml kubectl apply -f web-frontend.yaml Confirm that: Two pods are running, one for each service Each pod consists of two containers, the one running the service image, plus the envoy sidecar kubectl get pod Verify access to each service \u00b6 Let's deploy a pod that runs a curl image so we can verify that each service is reachable from within the cluster. The istio distribution comes with a sample called sleep that will serve this purpose. Deploy sleep to the default namespace. sleep.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 # Copyright Istio Authors # # Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. ################################################################################################## # Sleep service ################################################################################################## apiVersion : v1 kind : ServiceAccount metadata : name : sleep --- apiVersion : v1 kind : Service metadata : name : sleep labels : app : sleep service : sleep spec : ports : - port : 80 name : http selector : app : sleep --- apiVersion : apps/v1 kind : Deployment metadata : name : sleep spec : replicas : 1 selector : matchLabels : app : sleep template : metadata : labels : app : sleep spec : terminationGracePeriodSeconds : 0 serviceAccountName : sleep containers : - name : sleep image : curlimages/curl command : [ \"/bin/sleep\" , \"3650d\" ] imagePullPolicy : IfNotPresent volumeMounts : - mountPath : /etc/sleep/tls name : secret-volume volumes : - name : secret-volume secret : secretName : sleep-secret optional : true --- kubectl apply -f sleep.yaml Capture the name of the sleep pod to an environment variable SLEEP_POD = $( kubectl get pod -l app = sleep -ojsonpath = '{.items[0].metadata.name}' ) Use the kubectl exec command to call the customer service. kubectl exec $SLEEP_POD -it -- curl customers The console output should show a list of customers in JSON format. Call the web-frontend service kubectl exec $SLEEP_POD -it -- curl web-frontend The console output should show an HTML page listing customers using an HTML table. Next \u00b6 In the next lab, we expose the web-frontend using an Istio Ingress Gateway. This will allow us to see this application in a web browser.","title":"The application"},{"location":"the-app/#the-application","text":"In this lab you will deploy an application to your mesh. The application consists of two microservers, web-frontend and customers . Info The official Istio docs canonical example is the BookInfo aplpication . For this workshop we felt that an application involving fewer microservices would be more clear. The customers service exposes a REST endpoint that returns a list of customers in JSON format. The web-frontend calls customers retrieves the list, and uses the informationt to render the customer listing in HTML. The respective docker images for these services have already been built and pushed to a docker repository. You will deploy the application to the default Kubernetes namespace. But before proceeding, we must enable sidecar injection.","title":"The application"},{"location":"the-app/#enable-automatic-sidecar-injection","text":"There are two options for sidecar injection : automatic and manual. In this lab we will use automatic injection, which involves labeling the namespace where the pods are to reside. Label the default namespace kubectl label namespace default istio-injection = enabled Verify that the label has been applied: kubectl get ns -Listio-injection You can list the mutating webhooks in your kubernetes cluster and confirm that the sidecar injector is present. kubectl get mutatingwebhookconfigurations If you have extra time, explore the istioctl kube-inject command.","title":"Enable automatic sidecar injection"},{"location":"the-app/#deploy-the-application","text":"Study the two kubernetes yaml files: web-frontend.yaml and customers.yaml . web-frontend.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 --- apiVersion : v1 kind : ServiceAccount metadata : name : web-frontend --- apiVersion : apps/v1 kind : Deployment metadata : name : web-frontend labels : app : web-frontend spec : replicas : 1 selector : matchLabels : app : web-frontend template : metadata : labels : app : web-frontend version : v1 spec : serviceAccountName : web-frontend containers : - image : gcr.io/tetratelabs/web-frontend:1.0.0 imagePullPolicy : Always name : web ports : - containerPort : 8080 env : - name : CUSTOMER_SERVICE_URL value : \"http://customers.default.svc.cluster.local\" --- kind : Service apiVersion : v1 metadata : name : web-frontend labels : app : web-frontend spec : selector : app : web-frontend ports : - port : 80 name : http targetPort : 8080 customers.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 --- apiVersion : v1 kind : ServiceAccount metadata : name : customers --- apiVersion : apps/v1 kind : Deployment metadata : name : customers-v1 labels : app : customers version : v1 spec : replicas : 1 selector : matchLabels : app : customers version : v1 template : metadata : labels : app : customers version : v1 spec : serviceAccountName : customers containers : - image : gcr.io/tetratelabs/customers:1.0.0 imagePullPolicy : Always name : svc ports : - containerPort : 3000 --- kind : Service apiVersion : v1 metadata : name : customers labels : app : customers spec : selector : app : customers ports : - port : 80 name : http targetPort : 3000 Each file defines its corresponding deployment, service account, and ClusterIP service. Apply the two files to your Kubernetes cluster. kubectl apply -f customers.yaml kubectl apply -f web-frontend.yaml Confirm that: Two pods are running, one for each service Each pod consists of two containers, the one running the service image, plus the envoy sidecar kubectl get pod","title":"Deploy the application"},{"location":"the-app/#verify-access-to-each-service","text":"Let's deploy a pod that runs a curl image so we can verify that each service is reachable from within the cluster. The istio distribution comes with a sample called sleep that will serve this purpose. Deploy sleep to the default namespace. sleep.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 # Copyright Istio Authors # # Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. ################################################################################################## # Sleep service ################################################################################################## apiVersion : v1 kind : ServiceAccount metadata : name : sleep --- apiVersion : v1 kind : Service metadata : name : sleep labels : app : sleep service : sleep spec : ports : - port : 80 name : http selector : app : sleep --- apiVersion : apps/v1 kind : Deployment metadata : name : sleep spec : replicas : 1 selector : matchLabels : app : sleep template : metadata : labels : app : sleep spec : terminationGracePeriodSeconds : 0 serviceAccountName : sleep containers : - name : sleep image : curlimages/curl command : [ \"/bin/sleep\" , \"3650d\" ] imagePullPolicy : IfNotPresent volumeMounts : - mountPath : /etc/sleep/tls name : secret-volume volumes : - name : secret-volume secret : secretName : sleep-secret optional : true --- kubectl apply -f sleep.yaml Capture the name of the sleep pod to an environment variable SLEEP_POD = $( kubectl get pod -l app = sleep -ojsonpath = '{.items[0].metadata.name}' ) Use the kubectl exec command to call the customer service. kubectl exec $SLEEP_POD -it -- curl customers The console output should show a list of customers in JSON format. Call the web-frontend service kubectl exec $SLEEP_POD -it -- curl web-frontend The console output should show an HTML page listing customers using an HTML table.","title":"Verify access to each service"},{"location":"the-app/#next","text":"In the next lab, we expose the web-frontend using an Istio Ingress Gateway. This will allow us to see this application in a web browser.","title":"Next"},{"location":"traffic-shifting/","text":"Traffic shifting \u00b6","title":"Traffic shifting"},{"location":"traffic-shifting/#traffic-shifting","text":"","title":"Traffic shifting"}]}